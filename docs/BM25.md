# BM25 baseline

This readme contains instructions for reproducing a BM25 run using pyserini for the TREC track on tip-of-the-tongue (ToT) 
retrieval. You can view [Guidelines](https://trec-tot.github.io/guidelines) for more details.

## Baseline

To run the BM25 baseline:
```
DATA_PATH= ## enter path to data ###
python bm25.py --index_name bm25_0.8_1.0 --index_path ./anserini_indicies \
               --data_path $DATA_PATH \
               --param_k1 0.8 --param_b 1.0 \
               --field text --query title_text --split train \
               --run ./runs/bm25/train.run --run_format trec_eval --run_id baseline_bm25 \
               --negatives_out ./bm25_negatives/train-title_text-negatives.json
python bm25.py --index_name bm25_0.8_1.0 --index_path ./anserini_indicies \
               --data_path $DATA_PATH \
               --param_k1 0.8 --param_b 1.0 \
               --field text --query title_text --split dev \
               --run ./runs/bm25/dev.run --run_format trec_eval --run_id baseline_bm25 \
               --negatives_out ./bm25_negatives/dev-title_text-negatives.json
``` 

You can evaluate (on the dev set) either using pytrec_eval (included in the script above), or use `trec_eval`:

```
./trec_eval -m ndcg  -m recall.1,10,100,1000  -m recip_rank $DATA_PATH/dev/qrel.txt ./runs/bm25/dev.run
# output:
recip_rank            	all	0.0881
recall_1              	all	0.0800
recall_10             	all	0.0933
recall_100            	all	0.1800
recall_1000           	all	0.4067
ndcg                  	all	0.1314
```